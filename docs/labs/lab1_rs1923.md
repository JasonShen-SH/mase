# Varying Parameters

|           | max-epoch | learning-rate | batch-size |val_acc_last_epoch|
|-----------|-----------|---------------|------------|------------------|
| model1    | 10        | 1e-5          | 256        |      0.513       |
| model2    | 50        | 1e-5          | 256        |      0.618       |
| model3    | 100       | 1e-5          | 256        |      0.678       |
| model4    | 10        | 1e-5          | 16         |      0.569       |
| model5    | 10        | 1e-5          | 32         |      0.572       |
| model6    | 10        | 1e-5          | 64         |      0.550       |
| model7    | 10        | 1e-5          | 128        |      0.531       |
| model8    | 10        | 1e-5          | 512        |      0.471       |
| model9    | 10        | 1e-6          | 256        |      0.513       |
| model10   | 10        | 1e-4          | 256        |      0.680       |
| model11   | 10        | 1e-3          | 256        |      0.713       |
| model12   | 10        | 1e-2          | 256        |      0.573       |

For each model with specific parameters (batch size, maximum number of epochs, learning rate, etc.), we conducted 10 training iterations, each capped at a predefined number of epochs. ï¼ˆTo clarify, if each training iteration lasts for 50 epochs, and we conducted 10 separate training iterations, it amounted to a total of 500 epochs across all iterations.)__ 

Finally, we calculated the average validation accuracy using the values obtained at the final epoch of each independent training iteration, and this is represented by "**val_acc_last_epoch**".

## Impact of varying batch sizes
<img src="../../imgs/val_acc_batch_size.png" width=300>
After conducting experiments with batch sizes of 16, 32, 64, 128, 256, and 512, we have derived the following insights:       
Our obeservations indicate that an increase in batch size correlates with a decrease in validation accuracy. This phenomenon can be attributed to the fact that larger batch sizes provide a more comprehensive representation of the training dataset, leading to higher training accuracy. However, this comprehensive representation would normally have much higher training accuracy, which subsequently facilitate convergence towards local minima (as weight updates become less frequent).

Conversely, smaller batch sizes provide gradient estimates with higher variance, which is beneficial in that it enables the model to avoid the local minima, guiding it towards more optimal solutions that are closer to the global minima. In our experiments, generally speaking, smaller batch size resulted in improved validation accuracy due to the more frequent and varied weight updates.

Nonetheless, it is important to note that excessively small batch sizes can introduce an excessive amount of noise into the gradient estimates, which leads to instability during training and degraded model performance. (In our case, when batch size is only 16, the performance degraded a little).

Besides the impact on validation accuracy, we've also found that smaller batch sizes might lead to better generalization ability. See the table below for comparison: 
|           | batch-size |val_acc_last_epoch|train_acc_last_step|
|-----------|------------|------------------|-------------------|
| model4    | 16         |      0.569       |       0.487       |
| model5    | 32         |      0.572       |       0.500       |
| model6    | 64         |      0.550       |       0.500       |
| model7    | 128        |      0.531       |       0.588       |
| model1    | 256        |      0.513       |       0.561       |
| model8    | 512        |      0.471       |       0.487       |
Here, we use "**train_acc_last_step**" to represent the accuracy of the last training step. (This metric is not particularly accurate due to its randomness, but it could be representative).

It has been observed that larger batch sizes typically yield higher training accuracy compared to validation accuracy. Conversely, this trend tends to invert with the reduction of batch sizes, which means better generalization ability.

## Impact of varying max epochs
<img src="../../imgs/val_acc_max_epoch.png" width=300>


