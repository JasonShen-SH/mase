# Varying Parameters

|           | max-epoch | learning-rate | batch-size |val_acc_last_epoch|
|-----------|-----------|---------------|------------|------------------|
| model1    | 10        | 1e-5          | 256        |      0.513       |
| model2    | 50        | 1e-5          | 256        |      0.618       |
| model3    | 100       | 1e-5          | 256        |      0.678       |
| model4    | 10        | 1e-5          | 16         |      0.569       |
| model5    | 10        | 1e-5          | 32         |      0.572       |
| model6    | 10        | 1e-5          | 64         |      0.550       |
| model7    | 10        | 1e-5          | 128        |      0.531       |
| model8    | 10        | 1e-5          | 512        |      0.471       |
| model9    | 10        | 1e-6          | 256        |      0.513       |
| model10   | 10        | 1e-4          | 256        |      0.680       |
| model11   | 10        | 1e-3          | 256        |      0.713       |
| model12   | 10        | 1e-2          | 256        |      0.573       |

For each model with specific parameters (batch size, maximum number of epochs, learning rate, etc.), we conducted 10 training iterations, each capped at a predefined number of epochs. ï¼ˆTo clarify, if each training iteration lasts for 50 epochs, and we conducted 10 separate training iterations, it amounted to a total of 500 epochs across all iterations.) 

Finally, we calculated the average validation accuracy using the values obtained at the final epoch of each independent training iteration, and this is represented by "**val_acc_last_epoch**".

## Impact of varying batch sizes
<img src="../../imgs/val_acc_batch_size.png" width=300>
We've tried batch size of 16,32,64,128,256,512, and have come to the foloowing conclusions:
We found that larger batch size led to lower validation accuracy. As larger batch size which represents the train set more completely, it would normally have larger train accuracy, which leads to convergence to local minima as the weights updates slowly. 
Meanwhile, smaller batch size usually provides noisier gradient estimates, which helps the model escape local minima and finally reaches close to global minima. When batch isze decreases to 1, it becomes stochastic gradient descent. In our case, smaller batch size number led to higher validation accuracy. **However**, when

We also found that smaller batch sizes also leads to better generalization ability. See the table below for comparison:
|           | batch-size |val_acc_last_epoch|train_acc_last_step|
|-----------|------------|------------------|-------------------|
| model4    | 16         |      0.569       |       0.487       |
| model5    | 32         |      0.572       |       0.500       |
| model6    | 64         |      0.550       |       0.500       |
| model7    | 128        |      0.531       |       0.588       |
| model1    | 256        |      0.513       |       0.561       |
| model8    | 512        |      0.471       |       0.487       |
Here, we use "**train_acc_last_step**" to represent the training acuuracy of the last step of training. (This metric is not particularly accurate due to its randomness, but it could be representative)


## Impact of varying max epochs
<img src="../../imgs/val_acc_max_epoch.png" width=300>


